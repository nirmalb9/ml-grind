{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.330618\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *Fill this in* \n",
    "\n",
    "expectation for fj term is 0.5 since its randomized between 0 and 1, there are 10 terms, and a random one is denoted as correct, so we have a fraction equal to about 0.5/5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.020774 analytic: -0.020774, relative error: 4.638633e-09\n",
      "numerical: 0.235284 analytic: 0.235284, relative error: 2.270673e-07\n",
      "numerical: 1.417116 analytic: 1.417116, relative error: 1.192873e-08\n",
      "numerical: 2.052060 analytic: 2.052060, relative error: 1.511464e-08\n",
      "numerical: 1.118652 analytic: 1.118652, relative error: 9.134620e-09\n",
      "numerical: 0.469407 analytic: 0.469407, relative error: 2.564356e-08\n",
      "numerical: 0.166864 analytic: 0.166864, relative error: 3.797564e-08\n",
      "numerical: -1.449454 analytic: -1.449454, relative error: 1.005428e-08\n",
      "numerical: -1.544220 analytic: -1.544220, relative error: 9.592847e-09\n",
      "numerical: 4.108431 analytic: 4.108431, relative error: 9.380143e-09\n",
      "numerical: -1.828505 analytic: -1.824794, relative error: 1.015764e-03\n",
      "numerical: -1.480965 analytic: -1.479856, relative error: 3.747962e-04\n",
      "numerical: 0.827152 analytic: 0.821735, relative error: 3.284983e-03\n",
      "numerical: 2.805477 analytic: 2.800045, relative error: 9.691794e-04\n",
      "numerical: -2.528247 analytic: -2.527190, relative error: 2.090491e-04\n",
      "numerical: -0.532792 analytic: -0.535405, relative error: 2.445977e-03\n",
      "numerical: -0.241322 analytic: -0.244506, relative error: 6.554443e-03\n",
      "numerical: 0.740665 analytic: 0.740394, relative error: 1.831393e-04\n",
      "numerical: -2.241171 analytic: -2.241192, relative error: 4.640793e-06\n",
      "numerical: 0.552541 analytic: 0.553190, relative error: 5.866804e-04\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.330618e+00 computed in 0.104341s\n",
      "vectorized loss: 2.330618e+00 computed in 0.005243s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "tuning",
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 763.460194\n",
      "iteration 100 / 1000: loss 461.442525\n",
      "iteration 200 / 1000: loss 279.958318\n",
      "iteration 300 / 1000: loss 169.942199\n",
      "iteration 400 / 1000: loss 103.665732\n",
      "iteration 500 / 1000: loss 63.570493\n",
      "iteration 600 / 1000: loss 39.255742\n",
      "iteration 700 / 1000: loss 24.587635\n",
      "iteration 800 / 1000: loss 15.725943\n",
      "iteration 900 / 1000: loss 10.302083\n",
      "iteration 0 / 1000: loss 764.630485\n",
      "iteration 100 / 1000: loss 462.132851\n",
      "iteration 200 / 1000: loss 280.416938\n",
      "iteration 300 / 1000: loss 170.224285\n",
      "iteration 400 / 1000: loss 103.978033\n",
      "iteration 500 / 1000: loss 63.593518\n",
      "iteration 600 / 1000: loss 39.396060\n",
      "iteration 700 / 1000: loss 24.675246\n",
      "iteration 800 / 1000: loss 15.810858\n",
      "iteration 900 / 1000: loss 10.337385\n",
      "iteration 0 / 1000: loss 774.993167\n",
      "iteration 100 / 1000: loss 468.137690\n",
      "iteration 200 / 1000: loss 284.015811\n",
      "iteration 300 / 1000: loss 172.519581\n",
      "iteration 400 / 1000: loss 105.239827\n",
      "iteration 500 / 1000: loss 64.464936\n",
      "iteration 600 / 1000: loss 39.877801\n",
      "iteration 700 / 1000: loss 24.973482\n",
      "iteration 800 / 1000: loss 15.961516\n",
      "iteration 900 / 1000: loss 10.576629\n",
      "iteration 0 / 1000: loss 775.949517\n",
      "iteration 100 / 1000: loss 469.307312\n",
      "iteration 200 / 1000: loss 284.401952\n",
      "iteration 300 / 1000: loss 172.813302\n",
      "iteration 400 / 1000: loss 105.331534\n",
      "iteration 500 / 1000: loss 64.514581\n",
      "iteration 600 / 1000: loss 39.816757\n",
      "iteration 700 / 1000: loss 24.903593\n",
      "iteration 800 / 1000: loss 15.880957\n",
      "iteration 900 / 1000: loss 10.538478\n",
      "iteration 0 / 1000: loss 775.665610\n",
      "iteration 100 / 1000: loss 468.444868\n",
      "iteration 200 / 1000: loss 284.262514\n",
      "iteration 300 / 1000: loss 172.660611\n",
      "iteration 400 / 1000: loss 105.238666\n",
      "iteration 500 / 1000: loss 64.496771\n",
      "iteration 600 / 1000: loss 39.758632\n",
      "iteration 700 / 1000: loss 24.881817\n",
      "iteration 800 / 1000: loss 15.932785\n",
      "iteration 900 / 1000: loss 10.475014\n",
      "iteration 0 / 1000: loss 763.662193\n",
      "iteration 100 / 1000: loss 461.948911\n",
      "iteration 200 / 1000: loss 279.862623\n",
      "iteration 300 / 1000: loss 170.208588\n",
      "iteration 400 / 1000: loss 103.673720\n",
      "iteration 500 / 1000: loss 63.650912\n",
      "iteration 600 / 1000: loss 39.227326\n",
      "iteration 700 / 1000: loss 24.604634\n",
      "iteration 800 / 1000: loss 15.730488\n",
      "iteration 900 / 1000: loss 10.393681\n",
      "iteration 0 / 1000: loss 765.316593\n",
      "iteration 100 / 1000: loss 462.354950\n",
      "iteration 200 / 1000: loss 280.375645\n",
      "iteration 300 / 1000: loss 170.204968\n",
      "iteration 400 / 1000: loss 103.800512\n",
      "iteration 500 / 1000: loss 63.662077\n",
      "iteration 600 / 1000: loss 39.420109\n",
      "iteration 700 / 1000: loss 24.571909\n",
      "iteration 800 / 1000: loss 15.730542\n",
      "iteration 900 / 1000: loss 10.306089\n",
      "iteration 0 / 1000: loss 776.216246\n",
      "iteration 100 / 1000: loss 469.546615\n",
      "iteration 200 / 1000: loss 284.767455\n",
      "iteration 300 / 1000: loss 173.177808\n",
      "iteration 400 / 1000: loss 105.533816\n",
      "iteration 500 / 1000: loss 64.618060\n",
      "iteration 600 / 1000: loss 39.993999\n",
      "iteration 700 / 1000: loss 25.035990\n",
      "iteration 800 / 1000: loss 15.898733\n",
      "iteration 900 / 1000: loss 10.491633\n",
      "iteration 0 / 1000: loss 757.495957\n",
      "iteration 100 / 1000: loss 457.900740\n",
      "iteration 200 / 1000: loss 277.996097\n",
      "iteration 300 / 1000: loss 168.911697\n",
      "iteration 400 / 1000: loss 102.942429\n",
      "iteration 500 / 1000: loss 63.128262\n",
      "iteration 600 / 1000: loss 39.093190\n",
      "iteration 700 / 1000: loss 24.458410\n",
      "iteration 800 / 1000: loss 15.558507\n",
      "iteration 900 / 1000: loss 10.274203\n",
      "iteration 0 / 1000: loss 767.408460\n",
      "iteration 100 / 1000: loss 463.830909\n",
      "iteration 200 / 1000: loss 281.039143\n",
      "iteration 300 / 1000: loss 170.766531\n",
      "iteration 400 / 1000: loss 104.124416\n",
      "iteration 500 / 1000: loss 63.818265\n",
      "iteration 600 / 1000: loss 39.435528\n",
      "iteration 700 / 1000: loss 24.740875\n",
      "iteration 800 / 1000: loss 15.813341\n",
      "iteration 900 / 1000: loss 10.459441\n",
      "iteration 0 / 1000: loss 776.840599\n",
      "iteration 100 / 1000: loss 469.101023\n",
      "iteration 200 / 1000: loss 284.377793\n",
      "iteration 300 / 1000: loss 172.777756\n",
      "iteration 400 / 1000: loss 105.382050\n",
      "iteration 500 / 1000: loss 64.573625\n",
      "iteration 600 / 1000: loss 39.959068\n",
      "iteration 700 / 1000: loss 24.993063\n",
      "iteration 800 / 1000: loss 15.927190\n",
      "iteration 900 / 1000: loss 10.451222\n",
      "iteration 0 / 1000: loss 770.271167\n",
      "iteration 100 / 1000: loss 465.308089\n",
      "iteration 200 / 1000: loss 282.063523\n",
      "iteration 300 / 1000: loss 171.536602\n",
      "iteration 400 / 1000: loss 104.512384\n",
      "iteration 500 / 1000: loss 64.062916\n",
      "iteration 600 / 1000: loss 39.634609\n",
      "iteration 700 / 1000: loss 24.790875\n",
      "iteration 800 / 1000: loss 15.921508\n",
      "iteration 900 / 1000: loss 10.383829\n",
      "iteration 0 / 1000: loss 759.805839\n",
      "iteration 100 / 1000: loss 459.482922\n",
      "iteration 200 / 1000: loss 278.451655\n",
      "iteration 300 / 1000: loss 169.119291\n",
      "iteration 400 / 1000: loss 103.253627\n",
      "iteration 500 / 1000: loss 63.289567\n",
      "iteration 600 / 1000: loss 39.014976\n",
      "iteration 700 / 1000: loss 24.520771\n",
      "iteration 800 / 1000: loss 15.631249\n",
      "iteration 900 / 1000: loss 10.366982\n",
      "iteration 0 / 1000: loss 778.335455\n",
      "iteration 100 / 1000: loss 470.308286\n",
      "iteration 200 / 1000: loss 285.182563\n",
      "iteration 300 / 1000: loss 173.405968\n",
      "iteration 400 / 1000: loss 105.732826\n",
      "iteration 500 / 1000: loss 64.728423\n",
      "iteration 600 / 1000: loss 40.108227\n",
      "iteration 700 / 1000: loss 25.033225\n",
      "iteration 800 / 1000: loss 16.041242\n",
      "iteration 900 / 1000: loss 10.512893\n",
      "iteration 0 / 1000: loss 782.045230\n",
      "iteration 100 / 1000: loss 472.639557\n",
      "iteration 200 / 1000: loss 286.699862\n",
      "iteration 300 / 1000: loss 174.165277\n",
      "iteration 400 / 1000: loss 106.217148\n",
      "iteration 500 / 1000: loss 65.072958\n",
      "iteration 600 / 1000: loss 40.133671\n",
      "iteration 700 / 1000: loss 25.175252\n",
      "iteration 800 / 1000: loss 15.999318\n",
      "iteration 900 / 1000: loss 10.528538\n",
      "iteration 0 / 1000: loss 785.080831\n",
      "iteration 100 / 1000: loss 474.382168\n",
      "iteration 200 / 1000: loss 287.455099\n",
      "iteration 300 / 1000: loss 174.533145\n",
      "iteration 400 / 1000: loss 106.481649\n",
      "iteration 500 / 1000: loss 65.091930\n",
      "iteration 600 / 1000: loss 40.278169\n",
      "iteration 700 / 1000: loss 25.212087\n",
      "iteration 800 / 1000: loss 16.041548\n",
      "iteration 900 / 1000: loss 10.557971\n",
      "iteration 0 / 1000: loss 786.268960\n",
      "iteration 100 / 1000: loss 475.854697\n",
      "iteration 200 / 1000: loss 288.358778\n",
      "iteration 300 / 1000: loss 175.264049\n",
      "iteration 400 / 1000: loss 106.945510\n",
      "iteration 500 / 1000: loss 65.478838\n",
      "iteration 600 / 1000: loss 40.454394\n",
      "iteration 700 / 1000: loss 25.295387\n",
      "iteration 800 / 1000: loss 16.175471\n",
      "iteration 900 / 1000: loss 10.676286\n",
      "iteration 0 / 1000: loss 774.685814\n",
      "iteration 100 / 1000: loss 468.509509\n",
      "iteration 200 / 1000: loss 284.280690\n",
      "iteration 300 / 1000: loss 172.776419\n",
      "iteration 400 / 1000: loss 105.415106\n",
      "iteration 500 / 1000: loss 64.542744\n",
      "iteration 600 / 1000: loss 39.909077\n",
      "iteration 700 / 1000: loss 24.892734\n",
      "iteration 800 / 1000: loss 15.986598\n",
      "iteration 900 / 1000: loss 10.497215\n",
      "iteration 0 / 1000: loss 765.245829\n",
      "iteration 100 / 1000: loss 462.778765\n",
      "iteration 200 / 1000: loss 280.650229\n",
      "iteration 300 / 1000: loss 170.670642\n",
      "iteration 400 / 1000: loss 104.073522\n",
      "iteration 500 / 1000: loss 63.846779\n",
      "iteration 600 / 1000: loss 39.357016\n",
      "iteration 700 / 1000: loss 24.736312\n",
      "iteration 800 / 1000: loss 15.816411\n",
      "iteration 900 / 1000: loss 10.339173\n",
      "iteration 0 / 1000: loss 771.397863\n",
      "iteration 100 / 1000: loss 466.290166\n",
      "iteration 200 / 1000: loss 282.783313\n",
      "iteration 300 / 1000: loss 171.763252\n",
      "iteration 400 / 1000: loss 104.667331\n",
      "iteration 500 / 1000: loss 64.093839\n",
      "iteration 600 / 1000: loss 39.644526\n",
      "iteration 700 / 1000: loss 24.822614\n",
      "iteration 800 / 1000: loss 15.824198\n",
      "iteration 900 / 1000: loss 10.484710\n",
      "iteration 0 / 1000: loss 783.390711\n",
      "iteration 100 / 1000: loss 472.847938\n",
      "iteration 200 / 1000: loss 286.739284\n",
      "iteration 300 / 1000: loss 174.230294\n",
      "iteration 400 / 1000: loss 106.167713\n",
      "iteration 500 / 1000: loss 65.098273\n",
      "iteration 600 / 1000: loss 40.190382\n",
      "iteration 700 / 1000: loss 25.148907\n",
      "iteration 800 / 1000: loss 16.056210\n",
      "iteration 900 / 1000: loss 10.602949\n",
      "iteration 0 / 1000: loss 776.813099\n",
      "iteration 100 / 1000: loss 469.888472\n",
      "iteration 200 / 1000: loss 284.967552\n",
      "iteration 300 / 1000: loss 173.252972\n",
      "iteration 400 / 1000: loss 105.607916\n",
      "iteration 500 / 1000: loss 64.753769\n",
      "iteration 600 / 1000: loss 39.976124\n",
      "iteration 700 / 1000: loss 24.996897\n",
      "iteration 800 / 1000: loss 15.982634\n",
      "iteration 900 / 1000: loss 10.522789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 773.082835\n",
      "iteration 100 / 1000: loss 467.092210\n",
      "iteration 200 / 1000: loss 282.926301\n",
      "iteration 300 / 1000: loss 171.847117\n",
      "iteration 400 / 1000: loss 104.767205\n",
      "iteration 500 / 1000: loss 64.309754\n",
      "iteration 600 / 1000: loss 39.766705\n",
      "iteration 700 / 1000: loss 24.922822\n",
      "iteration 800 / 1000: loss 15.788032\n",
      "iteration 900 / 1000: loss 10.495125\n",
      "iteration 0 / 1000: loss 766.072944\n",
      "iteration 100 / 1000: loss 463.483109\n",
      "iteration 200 / 1000: loss 281.257793\n",
      "iteration 300 / 1000: loss 170.857962\n",
      "iteration 400 / 1000: loss 104.143685\n",
      "iteration 500 / 1000: loss 63.831450\n",
      "iteration 600 / 1000: loss 39.469996\n",
      "iteration 700 / 1000: loss 24.791710\n",
      "iteration 800 / 1000: loss 15.726329\n",
      "iteration 900 / 1000: loss 10.356230\n",
      "iteration 0 / 1000: loss 769.843734\n",
      "iteration 100 / 1000: loss 465.467208\n",
      "iteration 200 / 1000: loss 282.353864\n",
      "iteration 300 / 1000: loss 171.516132\n",
      "iteration 400 / 1000: loss 104.599409\n",
      "iteration 500 / 1000: loss 64.052997\n",
      "iteration 600 / 1000: loss 39.609034\n",
      "iteration 700 / 1000: loss 24.760998\n",
      "iteration 800 / 1000: loss 15.861120\n",
      "iteration 900 / 1000: loss 10.466611\n",
      "iteration 0 / 1000: loss 784.948623\n",
      "iteration 100 / 1000: loss 474.286969\n",
      "iteration 200 / 1000: loss 287.626090\n",
      "iteration 300 / 1000: loss 174.812918\n",
      "iteration 400 / 1000: loss 106.406916\n",
      "iteration 500 / 1000: loss 65.254117\n",
      "iteration 600 / 1000: loss 40.379192\n",
      "iteration 700 / 1000: loss 25.207502\n",
      "iteration 800 / 1000: loss 16.140476\n",
      "iteration 900 / 1000: loss 10.484517\n",
      "iteration 0 / 1000: loss 770.471927\n",
      "iteration 100 / 1000: loss 465.663029\n",
      "iteration 200 / 1000: loss 282.391964\n",
      "iteration 300 / 1000: loss 171.531853\n",
      "iteration 400 / 1000: loss 104.604722\n",
      "iteration 500 / 1000: loss 64.142405\n",
      "iteration 600 / 1000: loss 39.645601\n",
      "iteration 700 / 1000: loss 24.804839\n",
      "iteration 800 / 1000: loss 15.794947\n",
      "iteration 900 / 1000: loss 10.456524\n",
      "iteration 0 / 1000: loss 764.631991\n",
      "iteration 100 / 1000: loss 462.323926\n",
      "iteration 200 / 1000: loss 280.398454\n",
      "iteration 300 / 1000: loss 170.677662\n",
      "iteration 400 / 1000: loss 103.815878\n",
      "iteration 500 / 1000: loss 63.751782\n",
      "iteration 600 / 1000: loss 39.302634\n",
      "iteration 700 / 1000: loss 24.645772\n",
      "iteration 800 / 1000: loss 15.782399\n",
      "iteration 900 / 1000: loss 10.387025\n",
      "iteration 0 / 1000: loss 765.326358\n",
      "iteration 100 / 1000: loss 462.856452\n",
      "iteration 200 / 1000: loss 280.399438\n",
      "iteration 300 / 1000: loss 170.419035\n",
      "iteration 400 / 1000: loss 103.977867\n",
      "iteration 500 / 1000: loss 63.689949\n",
      "iteration 600 / 1000: loss 39.411005\n",
      "iteration 700 / 1000: loss 24.653291\n",
      "iteration 800 / 1000: loss 15.798672\n",
      "iteration 900 / 1000: loss 10.398850\n",
      "iteration 0 / 1000: loss 772.986115\n",
      "iteration 100 / 1000: loss 466.957074\n",
      "iteration 200 / 1000: loss 283.053907\n",
      "iteration 300 / 1000: loss 172.185173\n",
      "iteration 400 / 1000: loss 104.898933\n",
      "iteration 500 / 1000: loss 64.346244\n",
      "iteration 600 / 1000: loss 39.744187\n",
      "iteration 700 / 1000: loss 24.911079\n",
      "iteration 800 / 1000: loss 15.876305\n",
      "iteration 900 / 1000: loss 10.509908\n",
      "iteration 0 / 1000: loss 773.679069\n",
      "iteration 100 / 1000: loss 466.842126\n",
      "iteration 200 / 1000: loss 283.142547\n",
      "iteration 300 / 1000: loss 171.846096\n",
      "iteration 400 / 1000: loss 104.922372\n",
      "iteration 500 / 1000: loss 64.206202\n",
      "iteration 600 / 1000: loss 39.583294\n",
      "iteration 700 / 1000: loss 24.826511\n",
      "iteration 800 / 1000: loss 15.811600\n",
      "iteration 900 / 1000: loss 10.442097\n",
      "iteration 0 / 1000: loss 768.848627\n",
      "iteration 100 / 1000: loss 464.493807\n",
      "iteration 200 / 1000: loss 281.649340\n",
      "iteration 300 / 1000: loss 171.084213\n",
      "iteration 400 / 1000: loss 104.388124\n",
      "iteration 500 / 1000: loss 63.995966\n",
      "iteration 600 / 1000: loss 39.578895\n",
      "iteration 700 / 1000: loss 24.681098\n",
      "iteration 800 / 1000: loss 15.759787\n",
      "iteration 900 / 1000: loss 10.378415\n",
      "iteration 0 / 1000: loss 767.848311\n",
      "iteration 100 / 1000: loss 464.766414\n",
      "iteration 200 / 1000: loss 281.606974\n",
      "iteration 300 / 1000: loss 171.115696\n",
      "iteration 400 / 1000: loss 104.364072\n",
      "iteration 500 / 1000: loss 63.923086\n",
      "iteration 600 / 1000: loss 39.497426\n",
      "iteration 700 / 1000: loss 24.758187\n",
      "iteration 800 / 1000: loss 15.722268\n",
      "iteration 900 / 1000: loss 10.369923\n",
      "iteration 0 / 1000: loss 781.851457\n",
      "iteration 100 / 1000: loss 473.377352\n",
      "iteration 200 / 1000: loss 286.888315\n",
      "iteration 300 / 1000: loss 174.184385\n",
      "iteration 400 / 1000: loss 106.156650\n",
      "iteration 500 / 1000: loss 65.039499\n",
      "iteration 600 / 1000: loss 40.286207\n",
      "iteration 700 / 1000: loss 25.149874\n",
      "iteration 800 / 1000: loss 16.046100\n",
      "iteration 900 / 1000: loss 10.555335\n",
      "iteration 0 / 1000: loss 765.107575\n",
      "iteration 100 / 1000: loss 461.982145\n",
      "iteration 200 / 1000: loss 280.108156\n",
      "iteration 300 / 1000: loss 170.137489\n",
      "iteration 400 / 1000: loss 103.754078\n",
      "iteration 500 / 1000: loss 63.679933\n",
      "iteration 600 / 1000: loss 39.309235\n",
      "iteration 700 / 1000: loss 24.646688\n",
      "iteration 800 / 1000: loss 15.792126\n",
      "iteration 900 / 1000: loss 10.344264\n",
      "iteration 0 / 1000: loss 768.565958\n",
      "iteration 100 / 1000: loss 464.516236\n",
      "iteration 200 / 1000: loss 281.478710\n",
      "iteration 300 / 1000: loss 171.097817\n",
      "iteration 400 / 1000: loss 104.326948\n",
      "iteration 500 / 1000: loss 63.928892\n",
      "iteration 600 / 1000: loss 39.493058\n",
      "iteration 700 / 1000: loss 24.741617\n",
      "iteration 800 / 1000: loss 15.804900\n",
      "iteration 900 / 1000: loss 10.361216\n",
      "iteration 0 / 1000: loss 768.484057\n",
      "iteration 100 / 1000: loss 465.014162\n",
      "iteration 200 / 1000: loss 281.624533\n",
      "iteration 300 / 1000: loss 171.138725\n",
      "iteration 400 / 1000: loss 104.318712\n",
      "iteration 500 / 1000: loss 63.961648\n",
      "iteration 600 / 1000: loss 39.611788\n",
      "iteration 700 / 1000: loss 24.669588\n",
      "iteration 800 / 1000: loss 15.803926\n",
      "iteration 900 / 1000: loss 10.406909\n",
      "iteration 0 / 1000: loss 784.834280\n",
      "iteration 100 / 1000: loss 474.068964\n",
      "iteration 200 / 1000: loss 287.494455\n",
      "iteration 300 / 1000: loss 174.777577\n",
      "iteration 400 / 1000: loss 106.498149\n",
      "iteration 500 / 1000: loss 65.292029\n",
      "iteration 600 / 1000: loss 40.345736\n",
      "iteration 700 / 1000: loss 25.206512\n",
      "iteration 800 / 1000: loss 16.100991\n",
      "iteration 900 / 1000: loss 10.670661\n",
      "iteration 0 / 1000: loss 760.958190\n",
      "iteration 100 / 1000: loss 459.919592\n",
      "iteration 200 / 1000: loss 278.816091\n",
      "iteration 300 / 1000: loss 169.289293\n",
      "iteration 400 / 1000: loss 103.421186\n",
      "iteration 500 / 1000: loss 63.415813\n",
      "iteration 600 / 1000: loss 39.076026\n",
      "iteration 700 / 1000: loss 24.488434\n",
      "iteration 800 / 1000: loss 15.712236\n",
      "iteration 900 / 1000: loss 10.344025\n",
      "iteration 0 / 1000: loss 764.388822\n",
      "iteration 100 / 1000: loss 462.192845\n",
      "iteration 200 / 1000: loss 280.234421\n",
      "iteration 300 / 1000: loss 170.043798\n",
      "iteration 400 / 1000: loss 103.722370\n",
      "iteration 500 / 1000: loss 63.631844\n",
      "iteration 600 / 1000: loss 39.340443\n",
      "iteration 700 / 1000: loss 24.630717\n",
      "iteration 800 / 1000: loss 15.731179\n",
      "iteration 900 / 1000: loss 10.370355\n",
      "iteration 0 / 1000: loss 771.299043\n",
      "iteration 100 / 1000: loss 465.892985\n",
      "iteration 200 / 1000: loss 282.671561\n",
      "iteration 300 / 1000: loss 171.815225\n",
      "iteration 400 / 1000: loss 104.811307\n",
      "iteration 500 / 1000: loss 64.227904\n",
      "iteration 600 / 1000: loss 39.682911\n",
      "iteration 700 / 1000: loss 24.840117\n",
      "iteration 800 / 1000: loss 15.834839\n",
      "iteration 900 / 1000: loss 10.484602\n",
      "iteration 0 / 1000: loss 783.562278\n",
      "iteration 100 / 1000: loss 473.841863\n",
      "iteration 200 / 1000: loss 287.185630\n",
      "iteration 300 / 1000: loss 174.617018\n",
      "iteration 400 / 1000: loss 106.515036\n",
      "iteration 500 / 1000: loss 65.243322\n",
      "iteration 600 / 1000: loss 40.296790\n",
      "iteration 700 / 1000: loss 25.271301\n",
      "iteration 800 / 1000: loss 16.173987\n",
      "iteration 900 / 1000: loss 10.552724\n",
      "iteration 0 / 1000: loss 775.728592\n",
      "iteration 100 / 1000: loss 468.427059\n",
      "iteration 200 / 1000: loss 284.054862\n",
      "iteration 300 / 1000: loss 172.610839\n",
      "iteration 400 / 1000: loss 105.309368\n",
      "iteration 500 / 1000: loss 64.460147\n",
      "iteration 600 / 1000: loss 39.991996\n",
      "iteration 700 / 1000: loss 24.933574\n",
      "iteration 800 / 1000: loss 15.952747\n",
      "iteration 900 / 1000: loss 10.487061\n",
      "iteration 0 / 1000: loss 768.848148\n",
      "iteration 100 / 1000: loss 463.817214\n",
      "iteration 200 / 1000: loss 281.185665\n",
      "iteration 300 / 1000: loss 170.878555\n",
      "iteration 400 / 1000: loss 104.246577\n",
      "iteration 500 / 1000: loss 63.894236\n",
      "iteration 600 / 1000: loss 39.462782\n",
      "iteration 700 / 1000: loss 24.679118\n",
      "iteration 800 / 1000: loss 15.784631\n",
      "iteration 900 / 1000: loss 10.454335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 771.202106\n",
      "iteration 100 / 1000: loss 466.885142\n",
      "iteration 200 / 1000: loss 283.001200\n",
      "iteration 300 / 1000: loss 172.027349\n",
      "iteration 400 / 1000: loss 104.937837\n",
      "iteration 500 / 1000: loss 64.237046\n",
      "iteration 600 / 1000: loss 39.739480\n",
      "iteration 700 / 1000: loss 24.938779\n",
      "iteration 800 / 1000: loss 15.844841\n",
      "iteration 900 / 1000: loss 10.465389\n",
      "iteration 0 / 1000: loss 761.516019\n",
      "iteration 100 / 1000: loss 460.703963\n",
      "iteration 200 / 1000: loss 279.032959\n",
      "iteration 300 / 1000: loss 169.611113\n",
      "iteration 400 / 1000: loss 103.416886\n",
      "iteration 500 / 1000: loss 63.382137\n",
      "iteration 600 / 1000: loss 39.251058\n",
      "iteration 700 / 1000: loss 24.597912\n",
      "iteration 800 / 1000: loss 15.660858\n",
      "iteration 900 / 1000: loss 10.313536\n",
      "iteration 0 / 1000: loss 781.281315\n",
      "iteration 100 / 1000: loss 472.354664\n",
      "iteration 200 / 1000: loss 286.298076\n",
      "iteration 300 / 1000: loss 173.883577\n",
      "iteration 400 / 1000: loss 106.115766\n",
      "iteration 500 / 1000: loss 65.075604\n",
      "iteration 600 / 1000: loss 40.202766\n",
      "iteration 700 / 1000: loss 25.078353\n",
      "iteration 800 / 1000: loss 16.044354\n",
      "iteration 900 / 1000: loss 10.580730\n",
      "iteration 0 / 1000: loss 766.793877\n",
      "iteration 100 / 1000: loss 463.888431\n",
      "iteration 200 / 1000: loss 281.456522\n",
      "iteration 300 / 1000: loss 171.047924\n",
      "iteration 400 / 1000: loss 104.258863\n",
      "iteration 500 / 1000: loss 63.961395\n",
      "iteration 600 / 1000: loss 39.431139\n",
      "iteration 700 / 1000: loss 24.658753\n",
      "iteration 800 / 1000: loss 15.848001\n",
      "iteration 900 / 1000: loss 10.364006\n",
      "iteration 0 / 1000: loss 767.090821\n",
      "iteration 100 / 1000: loss 463.231702\n",
      "iteration 200 / 1000: loss 280.826380\n",
      "iteration 300 / 1000: loss 170.578352\n",
      "iteration 400 / 1000: loss 104.128656\n",
      "iteration 500 / 1000: loss 63.728547\n",
      "iteration 600 / 1000: loss 39.334092\n",
      "iteration 700 / 1000: loss 24.616616\n",
      "iteration 800 / 1000: loss 15.773440\n",
      "iteration 900 / 1000: loss 10.374580\n",
      "iteration 0 / 1000: loss 774.569712\n",
      "iteration 100 / 1000: loss 468.087486\n",
      "iteration 200 / 1000: loss 283.711429\n",
      "iteration 300 / 1000: loss 172.508702\n",
      "iteration 400 / 1000: loss 105.091946\n",
      "iteration 500 / 1000: loss 64.424072\n",
      "iteration 600 / 1000: loss 39.780063\n",
      "iteration 700 / 1000: loss 24.976102\n",
      "iteration 800 / 1000: loss 15.918574\n",
      "iteration 900 / 1000: loss 10.451274\n",
      "iteration 0 / 1000: loss 769.955852\n",
      "iteration 100 / 1000: loss 465.261189\n",
      "iteration 200 / 1000: loss 282.257628\n",
      "iteration 300 / 1000: loss 171.519232\n",
      "iteration 400 / 1000: loss 104.561118\n",
      "iteration 500 / 1000: loss 64.104182\n",
      "iteration 600 / 1000: loss 39.624794\n",
      "iteration 700 / 1000: loss 24.809762\n",
      "iteration 800 / 1000: loss 15.875164\n",
      "iteration 900 / 1000: loss 10.470569\n",
      "iteration 0 / 1000: loss 772.893277\n",
      "iteration 100 / 1000: loss 467.083130\n",
      "iteration 200 / 1000: loss 283.126910\n",
      "iteration 300 / 1000: loss 172.043779\n",
      "iteration 400 / 1000: loss 104.821474\n",
      "iteration 500 / 1000: loss 64.247075\n",
      "iteration 600 / 1000: loss 39.719230\n",
      "iteration 700 / 1000: loss 24.800823\n",
      "iteration 800 / 1000: loss 15.931923\n",
      "iteration 900 / 1000: loss 10.444541\n",
      "iteration 0 / 1000: loss 768.882514\n",
      "iteration 100 / 1000: loss 464.815936\n",
      "iteration 200 / 1000: loss 281.856901\n",
      "iteration 300 / 1000: loss 171.243816\n",
      "iteration 400 / 1000: loss 104.398082\n",
      "iteration 500 / 1000: loss 63.947460\n",
      "iteration 600 / 1000: loss 39.566261\n",
      "iteration 700 / 1000: loss 24.756773\n",
      "iteration 800 / 1000: loss 15.787720\n",
      "iteration 900 / 1000: loss 10.414298\n",
      "iteration 0 / 1000: loss 766.846933\n",
      "iteration 100 / 1000: loss 463.289721\n",
      "iteration 200 / 1000: loss 280.903934\n",
      "iteration 300 / 1000: loss 170.708615\n",
      "iteration 400 / 1000: loss 104.185496\n",
      "iteration 500 / 1000: loss 63.891674\n",
      "iteration 600 / 1000: loss 39.457794\n",
      "iteration 700 / 1000: loss 24.725559\n",
      "iteration 800 / 1000: loss 15.719521\n",
      "iteration 900 / 1000: loss 10.382091\n",
      "iteration 0 / 1000: loss 783.361643\n",
      "iteration 100 / 1000: loss 473.508213\n",
      "iteration 200 / 1000: loss 287.055206\n",
      "iteration 300 / 1000: loss 174.273339\n",
      "iteration 400 / 1000: loss 106.265936\n",
      "iteration 500 / 1000: loss 65.226059\n",
      "iteration 600 / 1000: loss 40.273406\n",
      "iteration 700 / 1000: loss 25.246065\n",
      "iteration 800 / 1000: loss 16.079445\n",
      "iteration 900 / 1000: loss 10.536181\n",
      "iteration 0 / 1000: loss 786.039952\n",
      "iteration 100 / 1000: loss 474.689855\n",
      "iteration 200 / 1000: loss 287.851652\n",
      "iteration 300 / 1000: loss 174.941255\n",
      "iteration 400 / 1000: loss 106.686695\n",
      "iteration 500 / 1000: loss 65.340388\n",
      "iteration 600 / 1000: loss 40.427988\n",
      "iteration 700 / 1000: loss 25.144615\n",
      "iteration 800 / 1000: loss 16.163929\n",
      "iteration 900 / 1000: loss 10.639385\n",
      "iteration 0 / 1000: loss 775.329350\n",
      "iteration 100 / 1000: loss 468.901482\n",
      "iteration 200 / 1000: loss 284.356961\n",
      "iteration 300 / 1000: loss 172.911913\n",
      "iteration 400 / 1000: loss 105.481827\n",
      "iteration 500 / 1000: loss 64.637866\n",
      "iteration 600 / 1000: loss 39.925262\n",
      "iteration 700 / 1000: loss 24.947585\n",
      "iteration 800 / 1000: loss 15.914935\n",
      "iteration 900 / 1000: loss 10.497746\n",
      "iteration 0 / 1000: loss 777.265828\n",
      "iteration 100 / 1000: loss 469.836213\n",
      "iteration 200 / 1000: loss 284.783392\n",
      "iteration 300 / 1000: loss 173.048355\n",
      "iteration 400 / 1000: loss 105.529521\n",
      "iteration 500 / 1000: loss 64.633555\n",
      "iteration 600 / 1000: loss 39.897245\n",
      "iteration 700 / 1000: loss 25.019163\n",
      "iteration 800 / 1000: loss 16.014065\n",
      "iteration 900 / 1000: loss 10.425350\n",
      "iteration 0 / 1000: loss 771.473948\n",
      "iteration 100 / 1000: loss 466.747376\n",
      "iteration 200 / 1000: loss 282.976659\n",
      "iteration 300 / 1000: loss 171.883446\n",
      "iteration 400 / 1000: loss 104.820446\n",
      "iteration 500 / 1000: loss 64.228300\n",
      "iteration 600 / 1000: loss 39.714790\n",
      "iteration 700 / 1000: loss 24.903539\n",
      "iteration 800 / 1000: loss 15.887756\n",
      "iteration 900 / 1000: loss 10.458686\n",
      "iteration 0 / 1000: loss 786.392460\n",
      "iteration 100 / 1000: loss 474.886732\n",
      "iteration 200 / 1000: loss 287.798479\n",
      "iteration 300 / 1000: loss 174.924299\n",
      "iteration 400 / 1000: loss 106.576032\n",
      "iteration 500 / 1000: loss 65.289225\n",
      "iteration 600 / 1000: loss 40.306362\n",
      "iteration 700 / 1000: loss 25.163927\n",
      "iteration 800 / 1000: loss 16.137536\n",
      "iteration 900 / 1000: loss 10.569111\n",
      "iteration 0 / 1000: loss 762.604695\n",
      "iteration 100 / 1000: loss 460.773818\n",
      "iteration 200 / 1000: loss 279.157581\n",
      "iteration 300 / 1000: loss 169.773922\n",
      "iteration 400 / 1000: loss 103.383922\n",
      "iteration 500 / 1000: loss 63.463619\n",
      "iteration 600 / 1000: loss 39.201563\n",
      "iteration 700 / 1000: loss 24.515374\n",
      "iteration 800 / 1000: loss 15.689719\n",
      "iteration 900 / 1000: loss 10.353537\n",
      "iteration 0 / 1000: loss 777.930839\n",
      "iteration 100 / 1000: loss 469.847415\n",
      "iteration 200 / 1000: loss 284.801087\n",
      "iteration 300 / 1000: loss 172.901746\n",
      "iteration 400 / 1000: loss 105.361550\n",
      "iteration 500 / 1000: loss 64.597113\n",
      "iteration 600 / 1000: loss 40.041448\n",
      "iteration 700 / 1000: loss 24.894449\n",
      "iteration 800 / 1000: loss 15.901501\n",
      "iteration 900 / 1000: loss 10.453678\n",
      "iteration 0 / 1000: loss 771.902989\n",
      "iteration 100 / 1000: loss 466.929640\n",
      "iteration 200 / 1000: loss 282.929122\n",
      "iteration 300 / 1000: loss 172.016403\n",
      "iteration 400 / 1000: loss 104.914451\n",
      "iteration 500 / 1000: loss 64.197167\n",
      "iteration 600 / 1000: loss 39.657746\n",
      "iteration 700 / 1000: loss 24.840471\n",
      "iteration 800 / 1000: loss 15.820722\n",
      "iteration 900 / 1000: loss 10.438751\n",
      "iteration 0 / 1000: loss 785.418788\n",
      "iteration 100 / 1000: loss 474.723471\n",
      "iteration 200 / 1000: loss 287.844038\n",
      "iteration 300 / 1000: loss 174.816000\n",
      "iteration 400 / 1000: loss 106.636466\n",
      "iteration 500 / 1000: loss 65.443426\n",
      "iteration 600 / 1000: loss 40.346532\n",
      "iteration 700 / 1000: loss 25.265297\n",
      "iteration 800 / 1000: loss 16.147592\n",
      "iteration 900 / 1000: loss 10.514949\n",
      "iteration 0 / 1000: loss 785.417799\n",
      "iteration 100 / 1000: loss 474.898739\n",
      "iteration 200 / 1000: loss 287.910213\n",
      "iteration 300 / 1000: loss 174.871587\n",
      "iteration 400 / 1000: loss 106.617747\n",
      "iteration 500 / 1000: loss 65.316637\n",
      "iteration 600 / 1000: loss 40.271312\n",
      "iteration 700 / 1000: loss 25.250644\n",
      "iteration 800 / 1000: loss 16.139314\n",
      "iteration 900 / 1000: loss 10.622144\n",
      "iteration 0 / 1000: loss 782.774494\n",
      "iteration 100 / 1000: loss 473.380289\n",
      "iteration 200 / 1000: loss 287.309189\n",
      "iteration 300 / 1000: loss 174.448810\n",
      "iteration 400 / 1000: loss 106.462030\n",
      "iteration 500 / 1000: loss 65.092420\n",
      "iteration 600 / 1000: loss 40.302743\n",
      "iteration 700 / 1000: loss 25.203497\n",
      "iteration 800 / 1000: loss 16.068974\n",
      "iteration 900 / 1000: loss 10.530467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 774.071180\n",
      "iteration 100 / 1000: loss 467.614939\n",
      "iteration 200 / 1000: loss 283.490615\n",
      "iteration 300 / 1000: loss 172.364316\n",
      "iteration 400 / 1000: loss 105.019218\n",
      "iteration 500 / 1000: loss 64.406040\n",
      "iteration 600 / 1000: loss 39.811118\n",
      "iteration 700 / 1000: loss 24.910065\n",
      "iteration 800 / 1000: loss 15.908347\n",
      "iteration 900 / 1000: loss 10.378711\n",
      "iteration 0 / 1000: loss 768.101901\n",
      "iteration 100 / 1000: loss 464.583607\n",
      "iteration 200 / 1000: loss 281.503445\n",
      "iteration 300 / 1000: loss 171.227062\n",
      "iteration 400 / 1000: loss 104.310060\n",
      "iteration 500 / 1000: loss 63.991708\n",
      "iteration 600 / 1000: loss 39.491572\n",
      "iteration 700 / 1000: loss 24.707287\n",
      "iteration 800 / 1000: loss 15.841485\n",
      "iteration 900 / 1000: loss 10.392029\n",
      "iteration 0 / 1000: loss 776.982400\n",
      "iteration 100 / 1000: loss 469.278475\n",
      "iteration 200 / 1000: loss 284.521468\n",
      "iteration 300 / 1000: loss 172.767199\n",
      "iteration 400 / 1000: loss 105.233767\n",
      "iteration 500 / 1000: loss 64.631960\n",
      "iteration 600 / 1000: loss 39.851399\n",
      "iteration 700 / 1000: loss 25.032801\n",
      "iteration 800 / 1000: loss 16.019166\n",
      "iteration 900 / 1000: loss 10.483588\n",
      "iteration 0 / 1000: loss 770.325961\n",
      "iteration 100 / 1000: loss 465.819182\n",
      "iteration 200 / 1000: loss 282.539076\n",
      "iteration 300 / 1000: loss 171.633250\n",
      "iteration 400 / 1000: loss 104.693543\n",
      "iteration 500 / 1000: loss 64.207464\n",
      "iteration 600 / 1000: loss 39.710059\n",
      "iteration 700 / 1000: loss 24.800176\n",
      "iteration 800 / 1000: loss 15.881678\n",
      "iteration 900 / 1000: loss 10.394071\n",
      "iteration 0 / 1000: loss 774.730015\n",
      "iteration 100 / 1000: loss 467.573903\n",
      "iteration 200 / 1000: loss 283.779375\n",
      "iteration 300 / 1000: loss 172.242142\n",
      "iteration 400 / 1000: loss 105.054172\n",
      "iteration 500 / 1000: loss 64.453927\n",
      "iteration 600 / 1000: loss 39.738761\n",
      "iteration 700 / 1000: loss 24.907848\n",
      "iteration 800 / 1000: loss 15.962129\n",
      "iteration 900 / 1000: loss 10.512346\n",
      "iteration 0 / 1000: loss 763.000106\n",
      "iteration 100 / 1000: loss 460.866265\n",
      "iteration 200 / 1000: loss 279.440172\n",
      "iteration 300 / 1000: loss 169.981394\n",
      "iteration 400 / 1000: loss 103.515087\n",
      "iteration 500 / 1000: loss 63.485891\n",
      "iteration 600 / 1000: loss 39.267453\n",
      "iteration 700 / 1000: loss 24.622295\n",
      "iteration 800 / 1000: loss 15.714666\n",
      "iteration 900 / 1000: loss 10.326893\n",
      "iteration 0 / 1000: loss 765.792421\n",
      "iteration 100 / 1000: loss 463.219912\n",
      "iteration 200 / 1000: loss 280.924819\n",
      "iteration 300 / 1000: loss 170.522507\n",
      "iteration 400 / 1000: loss 104.145581\n",
      "iteration 500 / 1000: loss 63.880340\n",
      "iteration 600 / 1000: loss 39.392468\n",
      "iteration 700 / 1000: loss 24.700656\n",
      "iteration 800 / 1000: loss 15.800102\n",
      "iteration 900 / 1000: loss 10.308973\n",
      "iteration 0 / 1000: loss 768.552093\n",
      "iteration 100 / 1000: loss 464.110380\n",
      "iteration 200 / 1000: loss 281.567981\n",
      "iteration 300 / 1000: loss 171.064472\n",
      "iteration 400 / 1000: loss 104.140050\n",
      "iteration 500 / 1000: loss 63.885689\n",
      "iteration 600 / 1000: loss 39.379135\n",
      "iteration 700 / 1000: loss 24.742501\n",
      "iteration 800 / 1000: loss 15.804970\n",
      "iteration 900 / 1000: loss 10.415900\n",
      "iteration 0 / 1000: loss 768.615270\n",
      "iteration 100 / 1000: loss 464.713527\n",
      "iteration 200 / 1000: loss 281.719185\n",
      "iteration 300 / 1000: loss 171.222565\n",
      "iteration 400 / 1000: loss 104.283966\n",
      "iteration 500 / 1000: loss 64.006767\n",
      "iteration 600 / 1000: loss 39.554673\n",
      "iteration 700 / 1000: loss 24.755998\n",
      "iteration 800 / 1000: loss 15.800016\n",
      "iteration 900 / 1000: loss 10.342053\n",
      "iteration 0 / 1000: loss 773.036487\n",
      "iteration 100 / 1000: loss 466.860821\n",
      "iteration 200 / 1000: loss 283.067954\n",
      "iteration 300 / 1000: loss 172.015460\n",
      "iteration 400 / 1000: loss 104.868946\n",
      "iteration 500 / 1000: loss 64.247558\n",
      "iteration 600 / 1000: loss 39.856119\n",
      "iteration 700 / 1000: loss 24.890374\n",
      "iteration 800 / 1000: loss 15.886383\n",
      "iteration 900 / 1000: loss 10.432170\n",
      "iteration 0 / 1000: loss 767.916575\n",
      "iteration 100 / 1000: loss 464.530011\n",
      "iteration 200 / 1000: loss 281.498237\n",
      "iteration 300 / 1000: loss 170.946880\n",
      "iteration 400 / 1000: loss 104.396830\n",
      "iteration 500 / 1000: loss 63.902651\n",
      "iteration 600 / 1000: loss 39.494800\n",
      "iteration 700 / 1000: loss 24.756196\n",
      "iteration 800 / 1000: loss 15.845457\n",
      "iteration 900 / 1000: loss 10.428224\n",
      "iteration 0 / 1000: loss 768.716206\n",
      "iteration 100 / 1000: loss 464.256348\n",
      "iteration 200 / 1000: loss 281.673141\n",
      "iteration 300 / 1000: loss 171.032805\n",
      "iteration 400 / 1000: loss 104.394786\n",
      "iteration 500 / 1000: loss 64.035125\n",
      "iteration 600 / 1000: loss 39.417091\n",
      "iteration 700 / 1000: loss 24.842947\n",
      "iteration 800 / 1000: loss 15.766097\n",
      "iteration 900 / 1000: loss 10.396040\n",
      "iteration 0 / 1000: loss 779.502945\n",
      "iteration 100 / 1000: loss 471.526420\n",
      "iteration 200 / 1000: loss 285.646060\n",
      "iteration 300 / 1000: loss 173.651824\n",
      "iteration 400 / 1000: loss 105.829763\n",
      "iteration 500 / 1000: loss 64.774983\n",
      "iteration 600 / 1000: loss 40.129979\n",
      "iteration 700 / 1000: loss 25.038693\n",
      "iteration 800 / 1000: loss 16.066407\n",
      "iteration 900 / 1000: loss 10.582274\n",
      "iteration 0 / 1000: loss 763.674099\n",
      "iteration 100 / 1000: loss 461.291377\n",
      "iteration 200 / 1000: loss 279.737721\n",
      "iteration 300 / 1000: loss 170.046863\n",
      "iteration 400 / 1000: loss 103.582596\n",
      "iteration 500 / 1000: loss 63.625151\n",
      "iteration 600 / 1000: loss 39.242364\n",
      "iteration 700 / 1000: loss 24.581033\n",
      "iteration 800 / 1000: loss 15.745526\n",
      "iteration 900 / 1000: loss 10.416989\n",
      "iteration 0 / 1000: loss 778.146572\n",
      "iteration 100 / 1000: loss 470.445891\n",
      "iteration 200 / 1000: loss 285.267386\n",
      "iteration 300 / 1000: loss 173.272344\n",
      "iteration 400 / 1000: loss 105.761896\n",
      "iteration 500 / 1000: loss 64.770564\n",
      "iteration 600 / 1000: loss 40.053252\n",
      "iteration 700 / 1000: loss 25.076975\n",
      "iteration 800 / 1000: loss 15.987028\n",
      "iteration 900 / 1000: loss 10.503852\n",
      "iteration 0 / 1000: loss 780.697396\n",
      "iteration 100 / 1000: loss 471.908605\n",
      "iteration 200 / 1000: loss 285.969625\n",
      "iteration 300 / 1000: loss 173.673692\n",
      "iteration 400 / 1000: loss 105.903692\n",
      "iteration 500 / 1000: loss 64.963397\n",
      "iteration 600 / 1000: loss 40.150334\n",
      "iteration 700 / 1000: loss 25.132499\n",
      "iteration 800 / 1000: loss 15.982151\n",
      "iteration 900 / 1000: loss 10.557937\n",
      "iteration 0 / 1000: loss 780.873690\n",
      "iteration 100 / 1000: loss 471.577653\n",
      "iteration 200 / 1000: loss 285.927787\n",
      "iteration 300 / 1000: loss 173.721516\n",
      "iteration 400 / 1000: loss 105.796951\n",
      "iteration 500 / 1000: loss 64.810980\n",
      "iteration 600 / 1000: loss 40.065150\n",
      "iteration 700 / 1000: loss 25.092854\n",
      "iteration 800 / 1000: loss 16.098507\n",
      "iteration 900 / 1000: loss 10.456071\n",
      "iteration 0 / 1000: loss 763.755308\n",
      "iteration 100 / 1000: loss 462.025400\n",
      "iteration 200 / 1000: loss 280.215510\n",
      "iteration 300 / 1000: loss 170.372589\n",
      "iteration 400 / 1000: loss 103.764997\n",
      "iteration 500 / 1000: loss 63.725776\n",
      "iteration 600 / 1000: loss 39.379466\n",
      "iteration 700 / 1000: loss 24.665785\n",
      "iteration 800 / 1000: loss 15.724586\n",
      "iteration 900 / 1000: loss 10.419927\n",
      "iteration 0 / 1000: loss 770.271234\n",
      "iteration 100 / 1000: loss 465.783551\n",
      "iteration 200 / 1000: loss 282.487821\n",
      "iteration 300 / 1000: loss 171.709502\n",
      "iteration 400 / 1000: loss 104.677001\n",
      "iteration 500 / 1000: loss 64.074459\n",
      "iteration 600 / 1000: loss 39.621016\n",
      "iteration 700 / 1000: loss 24.803041\n",
      "iteration 800 / 1000: loss 15.814485\n",
      "iteration 900 / 1000: loss 10.397280\n",
      "iteration 0 / 1000: loss 772.026745\n",
      "iteration 100 / 1000: loss 466.481969\n",
      "iteration 200 / 1000: loss 282.879043\n",
      "iteration 300 / 1000: loss 171.891912\n",
      "iteration 400 / 1000: loss 104.790163\n",
      "iteration 500 / 1000: loss 64.171225\n",
      "iteration 600 / 1000: loss 39.696199\n",
      "iteration 700 / 1000: loss 24.869737\n",
      "iteration 800 / 1000: loss 15.838102\n",
      "iteration 900 / 1000: loss 10.437709\n",
      "iteration 0 / 1000: loss 773.522295\n",
      "iteration 100 / 1000: loss 467.902839\n",
      "iteration 200 / 1000: loss 283.813832\n",
      "iteration 300 / 1000: loss 172.282873\n",
      "iteration 400 / 1000: loss 105.050532\n",
      "iteration 500 / 1000: loss 64.433672\n",
      "iteration 600 / 1000: loss 39.836565\n",
      "iteration 700 / 1000: loss 24.903143\n",
      "iteration 800 / 1000: loss 15.973032\n",
      "iteration 900 / 1000: loss 10.462132\n",
      "iteration 0 / 1000: loss 774.615379\n",
      "iteration 100 / 1000: loss 468.511335\n",
      "iteration 200 / 1000: loss 283.872668\n",
      "iteration 300 / 1000: loss 172.644553\n",
      "iteration 400 / 1000: loss 105.173955\n",
      "iteration 500 / 1000: loss 64.518772\n",
      "iteration 600 / 1000: loss 39.853776\n",
      "iteration 700 / 1000: loss 24.957349\n",
      "iteration 800 / 1000: loss 15.867341\n",
      "iteration 900 / 1000: loss 10.457997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 772.508610\n",
      "iteration 100 / 1000: loss 466.828435\n",
      "iteration 200 / 1000: loss 283.076804\n",
      "iteration 300 / 1000: loss 172.060335\n",
      "iteration 400 / 1000: loss 104.760946\n",
      "iteration 500 / 1000: loss 64.365471\n",
      "iteration 600 / 1000: loss 39.669671\n",
      "iteration 700 / 1000: loss 24.905813\n",
      "iteration 800 / 1000: loss 15.847382\n",
      "iteration 900 / 1000: loss 10.457858\n",
      "iteration 0 / 1000: loss 768.709740\n",
      "iteration 100 / 1000: loss 464.260245\n",
      "iteration 200 / 1000: loss 281.482161\n",
      "iteration 300 / 1000: loss 171.060436\n",
      "iteration 400 / 1000: loss 104.211775\n",
      "iteration 500 / 1000: loss 63.923005\n",
      "iteration 600 / 1000: loss 39.472013\n",
      "iteration 700 / 1000: loss 24.728573\n",
      "iteration 800 / 1000: loss 15.825905\n",
      "iteration 900 / 1000: loss 10.369754\n",
      "iteration 0 / 1000: loss 759.127797\n",
      "iteration 100 / 1000: loss 459.218571\n",
      "iteration 200 / 1000: loss 278.503306\n",
      "iteration 300 / 1000: loss 169.215955\n",
      "iteration 400 / 1000: loss 103.259993\n",
      "iteration 500 / 1000: loss 63.315629\n",
      "iteration 600 / 1000: loss 39.076419\n",
      "iteration 700 / 1000: loss 24.484330\n",
      "iteration 800 / 1000: loss 15.815814\n",
      "iteration 900 / 1000: loss 10.328060\n",
      "iteration 0 / 1000: loss 772.870328\n",
      "iteration 100 / 1000: loss 466.951935\n",
      "iteration 200 / 1000: loss 283.195967\n",
      "iteration 300 / 1000: loss 172.054298\n",
      "iteration 400 / 1000: loss 104.960278\n",
      "iteration 500 / 1000: loss 64.273946\n",
      "iteration 600 / 1000: loss 39.761878\n",
      "iteration 700 / 1000: loss 24.794165\n",
      "iteration 800 / 1000: loss 15.850061\n",
      "iteration 900 / 1000: loss 10.455517\n",
      "iteration 0 / 1000: loss 788.003790\n",
      "iteration 100 / 1000: loss 476.595029\n",
      "iteration 200 / 1000: loss 289.161666\n",
      "iteration 300 / 1000: loss 175.637137\n",
      "iteration 400 / 1000: loss 107.209924\n",
      "iteration 500 / 1000: loss 65.573105\n",
      "iteration 600 / 1000: loss 40.536939\n",
      "iteration 700 / 1000: loss 25.363679\n",
      "iteration 800 / 1000: loss 16.181352\n",
      "iteration 900 / 1000: loss 10.642763\n",
      "iteration 0 / 1000: loss 768.733568\n",
      "iteration 100 / 1000: loss 463.731876\n",
      "iteration 200 / 1000: loss 281.591820\n",
      "iteration 300 / 1000: loss 171.157485\n",
      "iteration 400 / 1000: loss 104.199491\n",
      "iteration 500 / 1000: loss 63.867689\n",
      "iteration 600 / 1000: loss 39.450808\n",
      "iteration 700 / 1000: loss 24.801793\n",
      "iteration 800 / 1000: loss 15.808663\n",
      "iteration 900 / 1000: loss 10.428747\n",
      "iteration 0 / 1000: loss 767.847488\n",
      "iteration 100 / 1000: loss 464.576400\n",
      "iteration 200 / 1000: loss 281.705023\n",
      "iteration 300 / 1000: loss 171.239631\n",
      "iteration 400 / 1000: loss 104.480809\n",
      "iteration 500 / 1000: loss 63.997008\n",
      "iteration 600 / 1000: loss 39.662334\n",
      "iteration 700 / 1000: loss 24.788693\n",
      "iteration 800 / 1000: loss 15.864039\n",
      "iteration 900 / 1000: loss 10.474639\n",
      "iteration 0 / 1000: loss 771.598676\n",
      "iteration 100 / 1000: loss 466.626821\n",
      "iteration 200 / 1000: loss 282.892288\n",
      "iteration 300 / 1000: loss 171.885297\n",
      "iteration 400 / 1000: loss 104.893299\n",
      "iteration 500 / 1000: loss 64.317625\n",
      "iteration 600 / 1000: loss 39.701745\n",
      "iteration 700 / 1000: loss 24.913676\n",
      "iteration 800 / 1000: loss 15.851632\n",
      "iteration 900 / 1000: loss 10.577611\n",
      "iteration 0 / 1000: loss 777.057049\n",
      "iteration 100 / 1000: loss 469.283993\n",
      "iteration 200 / 1000: loss 284.173904\n",
      "iteration 300 / 1000: loss 172.781570\n",
      "iteration 400 / 1000: loss 105.405470\n",
      "iteration 500 / 1000: loss 64.510657\n",
      "iteration 600 / 1000: loss 39.881237\n",
      "iteration 700 / 1000: loss 24.898907\n",
      "iteration 800 / 1000: loss 15.953691\n",
      "iteration 900 / 1000: loss 10.520780\n",
      "iteration 0 / 1000: loss 766.561552\n",
      "iteration 100 / 1000: loss 463.489936\n",
      "iteration 200 / 1000: loss 281.079086\n",
      "iteration 300 / 1000: loss 170.968812\n",
      "iteration 400 / 1000: loss 104.033274\n",
      "iteration 500 / 1000: loss 63.836643\n",
      "iteration 600 / 1000: loss 39.477013\n",
      "iteration 700 / 1000: loss 24.701780\n",
      "iteration 800 / 1000: loss 15.815359\n",
      "iteration 900 / 1000: loss 10.370082\n",
      "iteration 0 / 1000: loss 769.704073\n",
      "iteration 100 / 1000: loss 465.157812\n",
      "iteration 200 / 1000: loss 281.919728\n",
      "iteration 300 / 1000: loss 171.178523\n",
      "iteration 400 / 1000: loss 104.497406\n",
      "iteration 500 / 1000: loss 63.970602\n",
      "iteration 600 / 1000: loss 39.633347\n",
      "iteration 700 / 1000: loss 24.738929\n",
      "iteration 800 / 1000: loss 15.855694\n",
      "iteration 900 / 1000: loss 10.416224\n",
      "iteration 0 / 1000: loss 766.713916\n",
      "iteration 100 / 1000: loss 463.765898\n",
      "iteration 200 / 1000: loss 281.072735\n",
      "iteration 300 / 1000: loss 170.752948\n",
      "iteration 400 / 1000: loss 104.208572\n",
      "iteration 500 / 1000: loss 63.866694\n",
      "iteration 600 / 1000: loss 39.453173\n",
      "iteration 700 / 1000: loss 24.750554\n",
      "iteration 800 / 1000: loss 15.894219\n",
      "iteration 900 / 1000: loss 10.373030\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.339041 val accuracy: 0.350000\n",
      "lr 1.000000e-07 reg 2.678571e+04 train accuracy: 0.336939 val accuracy: 0.349000\n",
      "lr 1.000000e-07 reg 2.857143e+04 train accuracy: 0.337469 val accuracy: 0.359000\n",
      "lr 1.000000e-07 reg 3.035714e+04 train accuracy: 0.337469 val accuracy: 0.356000\n",
      "lr 1.000000e-07 reg 3.214286e+04 train accuracy: 0.342796 val accuracy: 0.362000\n",
      "lr 1.000000e-07 reg 3.392857e+04 train accuracy: 0.335000 val accuracy: 0.352000\n",
      "lr 1.000000e-07 reg 3.571429e+04 train accuracy: 0.335980 val accuracy: 0.349000\n",
      "lr 1.000000e-07 reg 3.750000e+04 train accuracy: 0.337531 val accuracy: 0.343000\n",
      "lr 1.000000e-07 reg 3.928571e+04 train accuracy: 0.335184 val accuracy: 0.336000\n",
      "lr 1.000000e-07 reg 4.107143e+04 train accuracy: 0.340449 val accuracy: 0.344000\n",
      "lr 1.285714e-07 reg 2.500000e+04 train accuracy: 0.337102 val accuracy: 0.351000\n",
      "lr 1.285714e-07 reg 2.678571e+04 train accuracy: 0.334429 val accuracy: 0.340000\n",
      "lr 1.285714e-07 reg 2.857143e+04 train accuracy: 0.342041 val accuracy: 0.366000\n",
      "lr 1.285714e-07 reg 3.035714e+04 train accuracy: 0.337673 val accuracy: 0.351000\n",
      "lr 1.285714e-07 reg 3.214286e+04 train accuracy: 0.333204 val accuracy: 0.347000\n",
      "lr 1.285714e-07 reg 3.392857e+04 train accuracy: 0.337163 val accuracy: 0.350000\n",
      "lr 1.285714e-07 reg 3.571429e+04 train accuracy: 0.338469 val accuracy: 0.372000\n",
      "lr 1.285714e-07 reg 3.750000e+04 train accuracy: 0.340857 val accuracy: 0.347000\n",
      "lr 1.285714e-07 reg 3.928571e+04 train accuracy: 0.335449 val accuracy: 0.349000\n",
      "lr 1.285714e-07 reg 4.107143e+04 train accuracy: 0.335122 val accuracy: 0.366000\n",
      "lr 1.571429e-07 reg 2.500000e+04 train accuracy: 0.339449 val accuracy: 0.343000\n",
      "lr 1.571429e-07 reg 2.678571e+04 train accuracy: 0.336245 val accuracy: 0.352000\n",
      "lr 1.571429e-07 reg 2.857143e+04 train accuracy: 0.337204 val accuracy: 0.358000\n",
      "lr 1.571429e-07 reg 3.035714e+04 train accuracy: 0.337184 val accuracy: 0.347000\n",
      "lr 1.571429e-07 reg 3.214286e+04 train accuracy: 0.342735 val accuracy: 0.362000\n",
      "lr 1.571429e-07 reg 3.392857e+04 train accuracy: 0.339265 val accuracy: 0.359000\n",
      "lr 1.571429e-07 reg 3.571429e+04 train accuracy: 0.337837 val accuracy: 0.339000\n",
      "lr 1.571429e-07 reg 3.750000e+04 train accuracy: 0.337653 val accuracy: 0.353000\n",
      "lr 1.571429e-07 reg 3.928571e+04 train accuracy: 0.339633 val accuracy: 0.355000\n",
      "lr 1.571429e-07 reg 4.107143e+04 train accuracy: 0.341510 val accuracy: 0.347000\n",
      "lr 1.857143e-07 reg 2.500000e+04 train accuracy: 0.335306 val accuracy: 0.339000\n",
      "lr 1.857143e-07 reg 2.678571e+04 train accuracy: 0.335122 val accuracy: 0.350000\n",
      "lr 1.857143e-07 reg 2.857143e+04 train accuracy: 0.338959 val accuracy: 0.354000\n",
      "lr 1.857143e-07 reg 3.035714e+04 train accuracy: 0.336061 val accuracy: 0.362000\n",
      "lr 1.857143e-07 reg 3.214286e+04 train accuracy: 0.339408 val accuracy: 0.346000\n",
      "lr 1.857143e-07 reg 3.392857e+04 train accuracy: 0.338204 val accuracy: 0.357000\n",
      "lr 1.857143e-07 reg 3.571429e+04 train accuracy: 0.339531 val accuracy: 0.348000\n",
      "lr 1.857143e-07 reg 3.750000e+04 train accuracy: 0.340959 val accuracy: 0.354000\n",
      "lr 1.857143e-07 reg 3.928571e+04 train accuracy: 0.337224 val accuracy: 0.351000\n",
      "lr 1.857143e-07 reg 4.107143e+04 train accuracy: 0.333714 val accuracy: 0.350000\n",
      "lr 2.142857e-07 reg 2.500000e+04 train accuracy: 0.339878 val accuracy: 0.340000\n",
      "lr 2.142857e-07 reg 2.678571e+04 train accuracy: 0.334653 val accuracy: 0.355000\n",
      "lr 2.142857e-07 reg 2.857143e+04 train accuracy: 0.338714 val accuracy: 0.353000\n",
      "lr 2.142857e-07 reg 3.035714e+04 train accuracy: 0.338000 val accuracy: 0.352000\n",
      "lr 2.142857e-07 reg 3.214286e+04 train accuracy: 0.336837 val accuracy: 0.350000\n",
      "lr 2.142857e-07 reg 3.392857e+04 train accuracy: 0.337776 val accuracy: 0.342000\n",
      "lr 2.142857e-07 reg 3.571429e+04 train accuracy: 0.338367 val accuracy: 0.335000\n",
      "lr 2.142857e-07 reg 3.750000e+04 train accuracy: 0.337673 val accuracy: 0.350000\n",
      "lr 2.142857e-07 reg 3.928571e+04 train accuracy: 0.337184 val accuracy: 0.357000\n",
      "lr 2.142857e-07 reg 4.107143e+04 train accuracy: 0.338776 val accuracy: 0.359000\n",
      "lr 2.428571e-07 reg 2.500000e+04 train accuracy: 0.342490 val accuracy: 0.358000\n",
      "lr 2.428571e-07 reg 2.678571e+04 train accuracy: 0.335408 val accuracy: 0.347000\n",
      "lr 2.428571e-07 reg 2.857143e+04 train accuracy: 0.339163 val accuracy: 0.335000\n",
      "lr 2.428571e-07 reg 3.035714e+04 train accuracy: 0.341898 val accuracy: 0.343000\n",
      "lr 2.428571e-07 reg 3.214286e+04 train accuracy: 0.334551 val accuracy: 0.336000\n",
      "lr 2.428571e-07 reg 3.392857e+04 train accuracy: 0.334347 val accuracy: 0.337000\n",
      "lr 2.428571e-07 reg 3.571429e+04 train accuracy: 0.336633 val accuracy: 0.333000\n",
      "lr 2.428571e-07 reg 3.750000e+04 train accuracy: 0.337796 val accuracy: 0.352000\n",
      "lr 2.428571e-07 reg 3.928571e+04 train accuracy: 0.338694 val accuracy: 0.359000\n",
      "lr 2.428571e-07 reg 4.107143e+04 train accuracy: 0.338041 val accuracy: 0.351000\n",
      "lr 2.714286e-07 reg 2.500000e+04 train accuracy: 0.336714 val accuracy: 0.341000\n",
      "lr 2.714286e-07 reg 2.678571e+04 train accuracy: 0.334388 val accuracy: 0.346000\n",
      "lr 2.714286e-07 reg 2.857143e+04 train accuracy: 0.338367 val accuracy: 0.338000\n",
      "lr 2.714286e-07 reg 3.035714e+04 train accuracy: 0.335878 val accuracy: 0.351000\n",
      "lr 2.714286e-07 reg 3.214286e+04 train accuracy: 0.337898 val accuracy: 0.353000\n",
      "lr 2.714286e-07 reg 3.392857e+04 train accuracy: 0.338510 val accuracy: 0.371000\n",
      "lr 2.714286e-07 reg 3.571429e+04 train accuracy: 0.337612 val accuracy: 0.360000\n",
      "lr 2.714286e-07 reg 3.750000e+04 train accuracy: 0.339020 val accuracy: 0.353000\n",
      "lr 2.714286e-07 reg 3.928571e+04 train accuracy: 0.340245 val accuracy: 0.362000\n",
      "lr 2.714286e-07 reg 4.107143e+04 train accuracy: 0.341673 val accuracy: 0.346000\n",
      "lr 3.000000e-07 reg 2.500000e+04 train accuracy: 0.339061 val accuracy: 0.348000\n",
      "lr 3.000000e-07 reg 2.678571e+04 train accuracy: 0.342143 val accuracy: 0.352000\n",
      "lr 3.000000e-07 reg 2.857143e+04 train accuracy: 0.337857 val accuracy: 0.338000\n",
      "lr 3.000000e-07 reg 3.035714e+04 train accuracy: 0.338265 val accuracy: 0.354000\n",
      "lr 3.000000e-07 reg 3.214286e+04 train accuracy: 0.338531 val accuracy: 0.354000\n",
      "lr 3.000000e-07 reg 3.392857e+04 train accuracy: 0.338041 val accuracy: 0.358000\n",
      "lr 3.000000e-07 reg 3.571429e+04 train accuracy: 0.337469 val accuracy: 0.362000\n",
      "lr 3.000000e-07 reg 3.750000e+04 train accuracy: 0.339714 val accuracy: 0.343000\n",
      "lr 3.000000e-07 reg 3.928571e+04 train accuracy: 0.333020 val accuracy: 0.345000\n",
      "lr 3.000000e-07 reg 4.107143e+04 train accuracy: 0.339571 val accuracy: 0.340000\n",
      "lr 3.285714e-07 reg 2.500000e+04 train accuracy: 0.336531 val accuracy: 0.331000\n",
      "lr 3.285714e-07 reg 2.678571e+04 train accuracy: 0.337469 val accuracy: 0.349000\n",
      "lr 3.285714e-07 reg 2.857143e+04 train accuracy: 0.338959 val accuracy: 0.346000\n",
      "lr 3.285714e-07 reg 3.035714e+04 train accuracy: 0.340306 val accuracy: 0.356000\n",
      "lr 3.285714e-07 reg 3.214286e+04 train accuracy: 0.336673 val accuracy: 0.343000\n",
      "lr 3.285714e-07 reg 3.392857e+04 train accuracy: 0.339551 val accuracy: 0.337000\n",
      "lr 3.285714e-07 reg 3.571429e+04 train accuracy: 0.339265 val accuracy: 0.349000\n",
      "lr 3.285714e-07 reg 3.750000e+04 train accuracy: 0.336265 val accuracy: 0.350000\n",
      "lr 3.285714e-07 reg 3.928571e+04 train accuracy: 0.335531 val accuracy: 0.354000\n",
      "lr 3.285714e-07 reg 4.107143e+04 train accuracy: 0.337020 val accuracy: 0.339000\n",
      "lr 3.571429e-07 reg 2.500000e+04 train accuracy: 0.340939 val accuracy: 0.349000\n",
      "lr 3.571429e-07 reg 2.678571e+04 train accuracy: 0.337286 val accuracy: 0.346000\n",
      "lr 3.571429e-07 reg 2.857143e+04 train accuracy: 0.338918 val accuracy: 0.355000\n",
      "lr 3.571429e-07 reg 3.035714e+04 train accuracy: 0.339061 val accuracy: 0.342000\n",
      "lr 3.571429e-07 reg 3.214286e+04 train accuracy: 0.337673 val accuracy: 0.353000\n",
      "lr 3.571429e-07 reg 3.392857e+04 train accuracy: 0.336980 val accuracy: 0.351000\n",
      "lr 3.571429e-07 reg 3.571429e+04 train accuracy: 0.339918 val accuracy: 0.368000\n",
      "lr 3.571429e-07 reg 3.750000e+04 train accuracy: 0.339735 val accuracy: 0.348000\n",
      "lr 3.571429e-07 reg 3.928571e+04 train accuracy: 0.338735 val accuracy: 0.334000\n",
      "lr 3.571429e-07 reg 4.107143e+04 train accuracy: 0.338531 val accuracy: 0.350000\n",
      "best validation accuracy achieved during cross-validation: 0.372000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "# Provided as a reference. You may or may not want to change these hyperparameters\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "reg_strengths = [2.5e4, 5e4]\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "learning_rate_splits = np.linspace(learning_rates[0],learning_rates[1],num=15)\n",
    "reg_splits = np.linspace(reg_strengths[0],reg_strengths[1],num=15)\n",
    "\n",
    "for i in range(10):\n",
    "    curr_lr = learning_rate_splits[i]\n",
    "    for j in range(10):\n",
    "        softmax = Softmax()\n",
    "        curr_reg = reg_splits[j]\n",
    "        loss_hist = softmax.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4, num_iters=1000, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "        results[(curr_lr,curr_reg)] = (train_acc,val_acc)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_softmax = softmax\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test"
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
